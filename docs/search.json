[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nDesign of Experiment\n",
    "section": "",
    "text": "Experimental design pertains to the way participants are assigned to various groups within an experiment. It is a thorough blueprint for gathering and utilizing data to uncover causal relationships. By meticulously strategizing, experimental design enables your data collection endeavors to effectively detect effects and assess hypotheses that address your research inquiries. Statistical design of experiments refers to the process of planning the experiment so that appropriate data will be collected and analyzed by statistical methods, resulting in valid and objective conclusions. Typically, the goal is to determine the impact that an independent variable has on a dependent variable. A well-designed experiment is important because the results and conclusions that can be drawn from the experiment depend to a largely on the way the data were collected. In experimental design it is important to identify the sources of variation. The specific one chosen in an experimental design will be determined by the design’s objective.\nA design is selected for an experiment based on the materials or treatments as well as the plots available for the experiment. Two main groups of designs are often used in breeding experiments. They are the complete block designs and the incomplete block designs.\nAll experiments have certain things in common, so designing an experiment usually includes the following steps. You must decide what question you want to have answered. This is the goal, or objective, of the experiment. The goal of the experiment will dictate what to include in the experiment to help you answer your question. The individual things that you wish to test in your experiment are called “treatments” and the physical areas to which the treatments are applied are called “plots.” Then you need to decide how the treatments should be physically arranged in the field. Technically, this is what is called the “experimental design.”\nThe objectives must be clearly stated as questions that need to be answered; hypotheses to be tested, and effects to be estimated. It is necessary to classify the objectives as major or minor since certain experimental designs give greater precision for some treatment comparisons than others.\n\n\nThere are three basic principles pf experimental design, these include:\n\nReplication: Two or more experiments using different experimental units, but with the same factor or independent variable settings. NB: Due to changes in lurking variables and inherent variations in experimental units, the measured dependent variable may differ among replicate runs. Replication entails duplicating experimental units or treatments to accommodate variability within the experiment. This practice enhances the accuracy of estimates and enables more robust statistical analysis. The number of replications is usually dependent on cost and time of experimentation. However, the smaller the size of difference expected to be detected, the higher the number of replicates required.\nReplication is necessary because all test plots are not identical, and that leads to variation in the data you collect; you will not get the same results from two plots that received the same treatment.\nRandomization: Is the process of randomly assigning treatments to experimental units. As a result of the random process, every treatment factors has the same probability. By randomly assigning experimental units to treatments, randomization enhances the reliability of experimental outcomes by minimizing the influence of extraneous variables. Randomization is the allocation of treatments to units such that the probability that a particular treatment will be allocated to a particular unit is the same for all treatments. In plant breeding experiments, two commonly employed randomization methods include Latin Square Designs and Randomized Complete Block Designs (RCBD).\nBlocking (error control): Is the procedure for gathering similar experimental units into a relatively homogeneous group. Blocking consists of grouping experimental units into homogeneous blocks according to factors that could affect the response variable. This approach mitigates experimental error by addressing variability related to specific factors, such as soil type or field location. Often blocking is used to reduce or eliminate the variability transmitted from nuisance factors—that is, factors that may influence the experimental response but in which we are not directly interested. In field experiments where substantial variation within an experimental field can be expected, significant reduction in experimental error is usually achieved with the use of proper blocking.\n\nIn plant breeding, we do consider another vital principle of experimental design. This is the interaction effects.\n\nInteraction Effects: Interaction happens when the impact of one independent variable varies depending on the level of another independent variable. Put differently, the effect of one independent variable is not consistent across all levels of the other independent variable. In plant breeding, interaction effects refer to how the performance or expression of a particular trait (dependent variable) varies depending on the combination of different factors or conditions (independent variables). These factors can include genetic traits (genotypes), environmental conditions (such as temperature, soil type, or moisture), management practices (like fertilizer application or pest control), or their combinations.\n\n\n\n\nExperimental design can be used in plant breeding such as:\n\nGenotype x Environment Interaction (G x E) studies: Genotype by Environment (G × E) interaction is crucial in plant breeding, as the performance of a genotype can differ across various environmental conditions. DoE allows breeders to evaluate G × E interaction through multi-locational trials conducted under diverse agro-climatic conditions. This approach aids in identifying genotypes that exhibit broad and specific adaptation, enabling breeders to develop cultivars that are more stable and productive.\nOptimization of Breeding: Experimental design enables the refinement of breeding strategies by assessing the effectiveness of different selection methods, breeding techniques, and combinations of traits. Through systematic testing of various breeding approaches, breeders can pinpoint the most efficient strategies to achieve specific breeding goals, such as enhancing disease resistance, improving yield, or increasing nutritional quality.\nOptimization of Field Management Practices: Aside from genetic factors, crop performance is significantly impacted by environmental management practices such as irrigation, fertilization, and pest control. Experimental design enables breeders to enhance field management practices through controlled experiments that evaluate the effects of various agronomic techniques on crop yield, quality, and resource utilization efficiency.\nStress Tolerance Screening: Given the escalating challenges presented by climate change and environmental pressures, there is an increasing demand to cultivate crop varieties that exhibit heightened stress tolerance. Experimental design supports stress tolerance screening by allowing breeders to methodically assess the performance of diverse germplasm under stressful conditions. By pinpointing genotypes with superior stress resilience and comprehending the underlying physiological mechanisms, breeders can cultivate resilient cultivars capable of thriving in challenging environments.\n\nThe application of experimental design in plant breeding can result in:\n\nImproved accuracy of decision making\nReduced variability\nReduced overall cost\nCompare resistant to drought\n\n\n\n\nThe following are the guidelines in designing of experiment\n\nDefine Objectives - Hypothesis\nIdentify Experimental Units\nDefine Measurable Response – Dependent Variable\nList Factor/Independent & Lurking Variables\nRun Pilot Tests\nMake a Flow Diagram of Experimental Procedure\nChoose Experimental Design\nDetermine the number of Replicates Required\nRandomize the Experimental Condition to Experimental Units\nDescribe a Method for Data Analysis\nTimetable and Budget for Resources Needed to Complete the Experiments\n\n\n\n\n\n\nExperiment (Run): A procedure in which the researcher modifies at least one of the variables under investigation and then examines the results.\nExperimental Unit (EU): Item that is being studied and on which something is modified. Experimental unit is the smallest thing that is measured. It may be a plot, an animal, a tree, group of trees, a plant, a person, etc., receiving a treatment.\nSub-Sample/Sub-Unit/Observational Unit: It is formed when the experimental unit is split after the action has been performed on it. NB: Sub-samples or sub-units of the same experimental unit are usually correlated and should be averaged before analysis.\nTreatment Factor (Independent Variable): Variable under investigation/control that is kept at near or perfect value, or level during experiment.\nBackground/Lurking Variable: Variable the experimenter is unaware of or unable to control that could affect the outcome of the experiment.\nResponse (Dependent Variable): The feature of the experimental unit that is measured after each experiment or runs. NB: The size of the responses is influenced by the settings of the independent variables or factors, as well as any hidden variables.\nExperimental Error: The variation between the observed response for a specific experiment and the long run average of all tests conducted with the same independent variables or factors. NB: Because of background or lurking variables, experimental errors are not equal to zero.\nEffect: The change in the response variable that is caused by change in the treatment factor or independent variable. NB: It’s termed calculated effect when it’s computed after an experiment from observed data, while it’s called effect size when it’s determined before an experiment.\nFixed Effect: We have a fixed effect if the treatments are well defined and easily replicable and are expected to yield the same impact on average in each replicate. The model is called a fixed effect model.\nRandom Effect: We have a random effect if the treatments cannot be considered to come from a predefined or known set, they are supposed to be a random sampling from a larger population of possible treatments. The model is called a random effects model.\nBlock Design: In many experiments, the available experimental units are grouped into blocks with similar characteristics to mitigate the impact of potential sources of variability. This approach is known as block design.\nComplete Block Design: The number of experimental units in a block is called the block size. If size of block is the same as the number of treatments and each treatment in each block is randomly allocated, then it is a full replication, and the design is called a complete block design.\n\nThe choice of experimental design to study the source of variability depends on the number of sources under study. The appropriate experimental design to study cause and effect relationship depends on:\n\nType & number of treatment factors\nDegree of homogeneity of experimental units\nEase of randomization\nAbility to block experimental units into homogeneous groups\n\nThe final experimental design chosen will determine the following\n\nHow data will be collected\nModel to be fit to analyze the data\nData interpretation\nConclusions drawn from experiment\n\nSome standard designs that are frequently used are:\n\nCompletely randomized design (CRD)\nRandomized complete block design (RCBD)\nIncomplete block design (IBD)\nLattice and Alpha design\nLatin Square Design\nAugmented design\nSplit plot design\nSparse Testing\nP-rep Design"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#principles-of-experimental-design",
    "href": "index.html#principles-of-experimental-design",
    "title": "\nDesign of Experiment\n",
    "section": "",
    "text": "There are three basic principles pf experimental design, these include:\n\nReplication: Two or more experiments using different experimental units, but with the same factor or independent variable settings. NB: Due to changes in lurking variables and inherent variations in experimental units, the measured dependent variable may differ among replicate runs. Replication entails duplicating experimental units or treatments to accommodate variability within the experiment. This practice enhances the accuracy of estimates and enables more robust statistical analysis. The number of replications is usually dependent on cost and time of experimentation. However, the smaller the size of difference expected to be detected, the higher the number of replicates required.\nReplication is necessary because all test plots are not identical, and that leads to variation in the data you collect; you will not get the same results from two plots that received the same treatment.\nRandomization: Is the process of randomly assigning treatments to experimental units. As a result of the random process, every treatment factors has the same probability. By randomly assigning experimental units to treatments, randomization enhances the reliability of experimental outcomes by minimizing the influence of extraneous variables. Randomization is the allocation of treatments to units such that the probability that a particular treatment will be allocated to a particular unit is the same for all treatments. In plant breeding experiments, two commonly employed randomization methods include Latin Square Designs and Randomized Complete Block Designs (RCBD).\nBlocking (error control): Is the procedure for gathering similar experimental units into a relatively homogeneous group. Blocking consists of grouping experimental units into homogeneous blocks according to factors that could affect the response variable. This approach mitigates experimental error by addressing variability related to specific factors, such as soil type or field location. Often blocking is used to reduce or eliminate the variability transmitted from nuisance factors—that is, factors that may influence the experimental response but in which we are not directly interested. In field experiments where substantial variation within an experimental field can be expected, significant reduction in experimental error is usually achieved with the use of proper blocking.\n\nIn plant breeding, we do consider another vital principle of experimental design. This is the interaction effects.\n\nInteraction Effects: Interaction happens when the impact of one independent variable varies depending on the level of another independent variable. Put differently, the effect of one independent variable is not consistent across all levels of the other independent variable. In plant breeding, interaction effects refer to how the performance or expression of a particular trait (dependent variable) varies depending on the combination of different factors or conditions (independent variables). These factors can include genetic traits (genotypes), environmental conditions (such as temperature, soil type, or moisture), management practices (like fertilizer application or pest control), or their combinations."
  },
  {
    "objectID": "index.html#applications-of-experimental-design-in-plant-breeding",
    "href": "index.html#applications-of-experimental-design-in-plant-breeding",
    "title": "\nDesign of Experiment\n",
    "section": "",
    "text": "Experimental design can be used in plant breeding such as:\n\nGenotype x Environment Interaction (G x E) studies: Genotype by Environment (G × E) interaction is crucial in plant breeding, as the performance of a genotype can differ across various environmental conditions. DoE allows breeders to evaluate G × E interaction through multi-locational trials conducted under diverse agro-climatic conditions. This approach aids in identifying genotypes that exhibit broad and specific adaptation, enabling breeders to develop cultivars that are more stable and productive.\nOptimization of Breeding: Experimental design enables the refinement of breeding strategies by assessing the effectiveness of different selection methods, breeding techniques, and combinations of traits. Through systematic testing of various breeding approaches, breeders can pinpoint the most efficient strategies to achieve specific breeding goals, such as enhancing disease resistance, improving yield, or increasing nutritional quality.\nOptimization of Field Management Practices: Aside from genetic factors, crop performance is significantly impacted by environmental management practices such as irrigation, fertilization, and pest control. Experimental design enables breeders to enhance field management practices through controlled experiments that evaluate the effects of various agronomic techniques on crop yield, quality, and resource utilization efficiency.\nStress Tolerance Screening: Given the escalating challenges presented by climate change and environmental pressures, there is an increasing demand to cultivate crop varieties that exhibit heightened stress tolerance. Experimental design supports stress tolerance screening by allowing breeders to methodically assess the performance of diverse germplasm under stressful conditions. By pinpointing genotypes with superior stress resilience and comprehending the underlying physiological mechanisms, breeders can cultivate resilient cultivars capable of thriving in challenging environments.\n\nThe application of experimental design in plant breeding can result in:\n\nImproved accuracy of decision making\nReduced variability\nReduced overall cost\nCompare resistant to drought"
  },
  {
    "objectID": "index.html#guidelines-for-designing-an-experiment",
    "href": "index.html#guidelines-for-designing-an-experiment",
    "title": "\nDesign of Experiment\n",
    "section": "",
    "text": "The following are the guidelines in designing of experiment\n\nDefine Objectives - Hypothesis\nIdentify Experimental Units\nDefine Measurable Response – Dependent Variable\nList Factor/Independent & Lurking Variables\nRun Pilot Tests\nMake a Flow Diagram of Experimental Procedure\nChoose Experimental Design\nDetermine the number of Replicates Required\nRandomize the Experimental Condition to Experimental Units\nDescribe a Method for Data Analysis\nTimetable and Budget for Resources Needed to Complete the Experiments"
  },
  {
    "objectID": "index.html#definition-of-terms",
    "href": "index.html#definition-of-terms",
    "title": "\nDesign of Experiment\n",
    "section": "",
    "text": "Experiment (Run): A procedure in which the researcher modifies at least one of the variables under investigation and then examines the results.\nExperimental Unit (EU): Item that is being studied and on which something is modified. Experimental unit is the smallest thing that is measured. It may be a plot, an animal, a tree, group of trees, a plant, a person, etc., receiving a treatment.\nSub-Sample/Sub-Unit/Observational Unit: It is formed when the experimental unit is split after the action has been performed on it. NB: Sub-samples or sub-units of the same experimental unit are usually correlated and should be averaged before analysis.\nTreatment Factor (Independent Variable): Variable under investigation/control that is kept at near or perfect value, or level during experiment.\nBackground/Lurking Variable: Variable the experimenter is unaware of or unable to control that could affect the outcome of the experiment.\nResponse (Dependent Variable): The feature of the experimental unit that is measured after each experiment or runs. NB: The size of the responses is influenced by the settings of the independent variables or factors, as well as any hidden variables.\nExperimental Error: The variation between the observed response for a specific experiment and the long run average of all tests conducted with the same independent variables or factors. NB: Because of background or lurking variables, experimental errors are not equal to zero.\nEffect: The change in the response variable that is caused by change in the treatment factor or independent variable. NB: It’s termed calculated effect when it’s computed after an experiment from observed data, while it’s called effect size when it’s determined before an experiment.\nFixed Effect: We have a fixed effect if the treatments are well defined and easily replicable and are expected to yield the same impact on average in each replicate. The model is called a fixed effect model.\nRandom Effect: We have a random effect if the treatments cannot be considered to come from a predefined or known set, they are supposed to be a random sampling from a larger population of possible treatments. The model is called a random effects model.\nBlock Design: In many experiments, the available experimental units are grouped into blocks with similar characteristics to mitigate the impact of potential sources of variability. This approach is known as block design.\nComplete Block Design: The number of experimental units in a block is called the block size. If size of block is the same as the number of treatments and each treatment in each block is randomly allocated, then it is a full replication, and the design is called a complete block design.\n\nThe choice of experimental design to study the source of variability depends on the number of sources under study. The appropriate experimental design to study cause and effect relationship depends on:\n\nType & number of treatment factors\nDegree of homogeneity of experimental units\nEase of randomization\nAbility to block experimental units into homogeneous groups\n\nThe final experimental design chosen will determine the following\n\nHow data will be collected\nModel to be fit to analyze the data\nData interpretation\nConclusions drawn from experiment\n\nSome standard designs that are frequently used are:\n\nCompletely randomized design (CRD)\nRandomized complete block design (RCBD)\nIncomplete block design (IBD)\nLattice and Alpha design\nLatin Square Design\nAugmented design\nSplit plot design\nSparse Testing\nP-rep Design"
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "\nDesign of Experiment\n",
    "section": "Example",
    "text": "Example\nThe basic R syntax for CRD is\n\n#design.crd(trt, r, serie = x, seed = yyy) #&lt;&lt;  \n\nArguments\n\ntrt : Treatments\nr : Replications\nserie : number plot, 1: 11,12; 2: 101,102; 3: 1001,1002\nseed : seed\n\nLets see an example\n\n#|message = FALSE\n#|eval=TRUE\n#Load libraries \nlibrary(tidyverse) \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(agricolae)  \n################Completely Randomized Design - CRD####################  \n#Planning CRD \ntrt &lt;- c(\"G1\",\"G2\",\"G3\",\"G4\",\"G5\") \nplan &lt;- design.crd(trt,3, seed=7638) \nplan &lt;- plan$book \nView(plan)  \n# write out the plan \nwrite_csv(plan, file = \"CRDplan.csv\")"
  },
  {
    "objectID": "index.html#example-1",
    "href": "index.html#example-1",
    "title": "\nDesign of Experiment\n",
    "section": "Example",
    "text": "Example\nThe basic R syntax for RCBD is\n\n#design.rcbd(trt, r, serie = x, seed = yyy) #&lt;&lt;\n\nArguments\n\ntrt : Treatments\nr : Replications or blocks\nserie : number plot, 1: 11,12; 2: 101,102; 3: 1001,1002\nseed : seed\n\nLets look at an example\n\n##############Randomized Complete Block Design - RCBD######################  #Planning RCBD \ntrt1 &lt;- c(\"A\",\"B\",\"C\",\"D\",\"E\") \nplan2 &lt;- design.rcbd(trt1, 4, seed = 161)\nplan2 &lt;- plan2$book \nView(plan2)   \n# write out the plan \nwrite_csv(plan2, file = \"RCBDplan.csv\")"
  },
  {
    "objectID": "index.html#example-of-square-lattice-design-sld",
    "href": "index.html#example-of-square-lattice-design-sld",
    "title": "\nDesign of Experiment\n",
    "section": "Example of Square Lattice Design (SLD)",
    "text": "Example of Square Lattice Design (SLD)\nThe basic R syntax for SLD is\n\n#design.lattice(trt2, r=x, seed = yyy)\n\nArguments\n\ntrt : treatments\nr : r = 2(simple) or r = 3(triple) lattice\nserie : number plot, 1: 11,12; 2: 101,102; 3: 1001,1002\nseed : seed\n\n\n#Planning Row Column Design - SLD \ntrt2 &lt;- 1:81 \nplan3 &lt;- design.lattice(trt2, r=2, seed = 123) \n\n\nLattice design,  simple   9 x 9 \n\nEfficiency factor\n(E ) 0.8333333 \n\n&lt;&lt;&lt; Book &gt;&gt;&gt;\n\nplan3 &lt;- plan3$book \nView(plan3)   # write out the plan write_csv(plan3, file = \"SLDplan.csv\")"
  },
  {
    "objectID": "index.html#example-of-alpha-design",
    "href": "index.html#example-of-alpha-design",
    "title": "\nDesign of Experiment\n",
    "section": "Example of Alpha design",
    "text": "Example of Alpha design\nThe basic R syntax for Alpha Design is\n\n#design.alpha(trt,k,r,serie=2, seed=seed)\n\nArguments\n\ntrt : treatments\nk : size block\nr : Replications\nserie : number plot, 1: 11,12; 2: 101,102; 3: 1001,1002\nseed : seed\n\n\n#Planning an Alpha design - AD \n#30 test materials \ntrt &lt;- 1:30 #trt &lt;- letters[1:12] \nt &lt;- length(trt) \n# size block k \nk &lt;- 3 \n# Blocks s \ns &lt;- t/k \n# replications r \nr &lt;- 2   \nplan4 &lt;- design.alpha(trt,k,r,serie=2, seed=1234)  \n\n\nAlpha Design (0,1) - Serie  I \n\nParameters Alpha Design\n=======================\nTreatmeans : 30\nBlock size : 3\nBlocks     : 10\nReplication: 2 \n\nEfficiency factor\n(E ) 0.6170213 \n\n&lt;&lt;&lt; Book &gt;&gt;&gt;\n\nplan4 &lt;- plan4$book  \nView(plan4)  \n# write out the plan\nwrite_csv(plan4, file = \"ADplan.csv\")"
  },
  {
    "objectID": "index.html#example-2",
    "href": "index.html#example-2",
    "title": "\nDesign of Experiment\n",
    "section": "Example",
    "text": "Example\nThe basic R syntax for LSD is\n\n#LSD.test(y, trt, DFerror, MSerror, alpha, group)\n\nArguments\n\ntrt: treatments\nserie : number plot, 1: 11,12; 2: 101,102; 3: 1001,1002\nseed : seed"
  },
  {
    "objectID": "index.html#example-3",
    "href": "index.html#example-3",
    "title": "\nDesign of Experiment\n",
    "section": "Example",
    "text": "Example\nThe basic R syntax to achieve this is:\n\n# design.dau(trt1, trt2, r, serie = 2, seed = 0, kinds = \"Super-Duper\", name=\"trt\",randomization=TRUE)\n\nArguments\n\ntrt1: check\ntrt2: treatment\nr: Replications or blocks\nserie: number plot, 1: 11,12; 2: 101,102; 3: 1001,1002\nseed: seed\n\nLets look at an example\n#Planning ABD \ncheck &lt;- c(\"A\",\"B\",\"C\",\"D\")  #Checks\nnew &lt;- letters[20:26]   #New treatments\n# 5 Replication or blocks \nplan5 &lt;-design.dau(check, new, r=5, serie=2, seed = 1611)\nplan5 &lt;- plan5$book\nView(plan5)  \n# write out the plan \nwrite_csv(plan5, file = \"ABDplan.csv\")"
  },
  {
    "objectID": "index.html#example-4",
    "href": "index.html#example-4",
    "title": "\nDesign of Experiment\n",
    "section": "Example",
    "text": "Example\ntrt1 &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\") \ntrt2 &lt;- c(\"N0\", \"N50\", \"N100\") \nplanSP &lt;- design.split(trt1, trt2, r=3, serie=2, seed=14) \nplanSP &lt;- planSP$book \nView(planSP)  \n# write out the plan\nwrite_csv(planSP, file = \"SPplan.csv\")"
  },
  {
    "objectID": "index.html#example-5",
    "href": "index.html#example-5",
    "title": "\nDesign of Experiment\n",
    "section": "Example",
    "text": "Example\n\n45 lines:\n\n10 lines replicated twice (20%-25%)\n35 unreplicated lines\n5 blocks\n3 checks in each of the 5 blocks\n\n\nNumber of plots: 10x2 + 35 + 3x5 = 70 plots"
  }
]